{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0045f78e-7156-4253-9a49-8de9d6220e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomResizedCrop(size=28, scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1] for grayscale\n",
    "])\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.FashionMNIST(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    ),\n",
    "    batch_size=128, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e17a92e-00bb-4475-a3dd-74c21ef01aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 128, 7, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 1, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 1, 7, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x).view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "039a8a55-84e0-4207-bb0b-75e3d80f7e11",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m netG \u001b[38;5;241m=\u001b[39m \u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m netD \u001b[38;5;241m=\u001b[39m Discriminator()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Loss and optimizers\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/ds340/students/leilanih/.conda/envs/GAN_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projectnb/ds340/students/leilanih/.conda/envs/GAN_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/ds340/students/leilanih/.conda/envs/GAN_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/ds340/students/leilanih/.conda/envs/GAN_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/projectnb/ds340/students/leilanih/.conda/envs/GAN_project_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "netG = Generator().to(device)\n",
    "netD = Discriminator().to(device)\n",
    "\n",
    "# Loss and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "z = 100\n",
    "lr = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4d99d47-5e7f-4ce2-bd13-7295ee5a84c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (real_images, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     12\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m real_images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     real_images \u001b[38;5;241m=\u001b[39m \u001b[43mreal_images\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     real_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m     fake_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    for i, (real_images, _) in enumerate(dataloader):\n",
    "        batch_size = real_images.size(0)\n",
    "        real_images = real_images.to(device)\n",
    "\n",
    "        real_labels = torch.ones(batch_size).to(device)\n",
    "        fake_labels = torch.zeros(batch_size).to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        netD.zero_grad()\n",
    "        outputs = netD(real_images)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "\n",
    "        z = torch.randn(batch_size, 100, 1, 1).to(device)\n",
    "        fake_images = netG(z)\n",
    "        outputs = netD(fake_images.detach())\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Train Generator\n",
    "        netG.zero_grad()\n",
    "        z = torch.randn(batch_size, 100, 1, 1).to(device)\n",
    "        fake_images = netG(z)\n",
    "        outputs = netD(fake_images)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/20], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "    g_losses.append((epoch + 1, g_loss.item()))\n",
    "    d_losses.append((epoch + 1, d_loss.item()))\n",
    "\n",
    "    # Show sample grid every 5 epochs\n",
    "    \"\"\"if (epoch + 1) % 5 == 0:\n",
    "        netG.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(64, 100, 1, 1).to(device)\n",
    "            fake_images = netG(z)\n",
    "            grid = make_grid(fake_images[:64], nrow=8, normalize=True)\n",
    "            plt.imshow(grid.permute(1, 2, 0).cpu())\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"Generated Samples at Epoch {epoch+1}\")\n",
    "            plt.show()\"\"\"\n",
    "    netG.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58ac2cfc-7ee2-40e5-957b-6707d14a2b40",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'netG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnetG\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      2\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m fake_images \u001b[38;5;241m=\u001b[39m netG(z)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'netG' is not defined"
     ]
    }
   ],
   "source": [
    "netG.eval()\n",
    "z = torch.randn(64, 100, 1, 1).to(device)\n",
    "fake_images = netG(z)\n",
    "grid = make_grid(fake_images[:64], nrow=8, normalize=True)\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu())\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6259f542-9e1b-40b6-b310-b34c229789d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58797a95-efef-4180-809b-876cb2bd5fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b56ccb1-4cce-4b92-869e-c9a068369224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set output folder\n",
    "output_dir = \"/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save real images\n",
    "real_images_list = []\n",
    "for idx, (real_img, _) in enumerate(dataloader):\n",
    "    real_images_list.append(real_img)  # still normalized [-1,1]\n",
    "    if idx >= 100:  # limit number of batches\n",
    "        break\n",
    "\n",
    "real_images_all = torch.cat(real_images_list, dim=0)  # shape (N, 1, 28, 28)\n",
    "real_images_all = (real_images_all + 1) / 2  # normalize to [0,1] for FID\n",
    "real_images_all = real_images_all.expand(-1, 3, -1, -1)  # repeat channels to make (N,3,28,28)\n",
    "real_images_all = real_images_all.numpy()\n",
    "np.save(os.path.join(output_dir, \"real_images_fid.npy\"), real_images_all)\n",
    "\n",
    "# Save fake images\n",
    "netG.eval()\n",
    "fake_images_list = []\n",
    "with torch.no_grad():\n",
    "    for idx in range(100):\n",
    "        z = torch.randn(batch_size, 100, 1, 1).to(device)\n",
    "        fake_imgs = netG(z)\n",
    "        fake_imgs = (fake_imgs + 1) / 2  # normalize to [0,1]\n",
    "        fake_imgs = fake_imgs.expand(-1, 3, -1, -1)  # make 3 channels\n",
    "        fake_images_list.append(fake_imgs.cpu())\n",
    "        \n",
    "fake_images_all = torch.cat(fake_images_list, dim=0)\n",
    "fake_images_all = fake_images_all.numpy()\n",
    "np.save(os.path.join(output_dir, \"fake_images_fid.npy\"), fake_images_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3f81411-15e1-4d48-93c1-7788acb5e482",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2234033974.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[45], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python fid_score.py --true /projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy --fake /projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_fid.npy --gpu 1\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python fid_score.py --true /projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy --fake /projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_fid.npy --gpu 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496267c3-b680-4bed-97b9-ac7c5c3b3c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05bffbf3-a582-47cf-9779-fd39c83a4376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], D Loss: 0.5116, G Loss: 1.2386\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_001.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [2/20], D Loss: 1.0105, G Loss: 1.0895\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_002.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [3/20], D Loss: 1.1562, G Loss: 0.7437\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_003.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [4/20], D Loss: 1.1021, G Loss: 0.8326\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_004.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [5/20], D Loss: 1.1450, G Loss: 1.2468\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_005.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [6/20], D Loss: 1.1321, G Loss: 0.7240\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_006.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [7/20], D Loss: 1.3677, G Loss: 1.5595\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_007.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [8/20], D Loss: 1.1106, G Loss: 1.1920\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_008.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [9/20], D Loss: 1.1728, G Loss: 0.8442\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_009.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [10/20], D Loss: 1.2256, G Loss: 1.5785\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_010.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [11/20], D Loss: 1.1603, G Loss: 0.9989\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_011.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [12/20], D Loss: 1.1726, G Loss: 1.1269\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_012.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [13/20], D Loss: 1.0382, G Loss: 1.2454\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_013.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [14/20], D Loss: 1.1733, G Loss: 0.6953\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_014.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [15/20], D Loss: 1.0760, G Loss: 0.7446\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_015.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [16/20], D Loss: 0.9287, G Loss: 1.2658\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_016.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [17/20], D Loss: 0.9927, G Loss: 0.8394\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_017.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [18/20], D Loss: 0.8655, G Loss: 1.4856\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_018.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [19/20], D Loss: 0.8527, G Loss: 1.4125\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_019.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n",
      "Epoch [20/20], D Loss: 0.8857, G Loss: 1.2297\n",
      "Namespace(true='/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy', fake=['/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_020.npy'], batch_size=50, dims=2048, gpu='0', model='inception')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fid_scores = []  # <--- store (epoch, fid_score)\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "for epoch in range(epochs):\n",
    "    for i, (real_images, _) in enumerate(dataloader):\n",
    "        batch_size = real_images.size(0)\n",
    "        real_images = real_images.to(device)\n",
    "\n",
    "        real_labels = torch.ones(batch_size).to(device)\n",
    "        fake_labels = torch.zeros(batch_size).to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        netD.zero_grad()\n",
    "        outputs = netD(real_images)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "\n",
    "        z = torch.randn(batch_size, 100, 1, 1).to(device)\n",
    "        fake_images = netG(z)\n",
    "        outputs = netD(fake_images.detach())\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Train Generator\n",
    "        netG.zero_grad()\n",
    "        z = torch.randn(batch_size, 100, 1, 1).to(device)\n",
    "        fake_images = netG(z)\n",
    "        outputs = netD(fake_images)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/20], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "    g_losses.append((epoch + 1, g_loss.item()))\n",
    "    d_losses.append((epoch + 1, d_loss.item()))\n",
    "\n",
    "    # Every N epochs, calculate FID\n",
    "    if (epoch + 1) % 1 == 0: \n",
    "        netG.eval()\n",
    "        with torch.no_grad():\n",
    "            # Generate fake images\n",
    "            z = torch.randn(1000, 100, 1, 1).to(device)\n",
    "            fake_images = netG(z)\n",
    "\n",
    "            # Save fake images\n",
    "            save_path = '/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/fake_images_epoch_{:03d}.npy'.format(epoch+1)\n",
    "            np.save(save_path, fake_images.cpu().numpy())\n",
    "\n",
    "            result = subprocess.run([\n",
    "                'python', 'fid_score.py',\n",
    "                '--true', '/projectnb/ds340/projects/leilani_hannah_final_project/fid_images_augmented/real_images_fid.npy',\n",
    "                '--fake', save_path,\n",
    "                '--gpu', '0'\n",
    "            ], capture_output=True, text=True)\n",
    "\n",
    "            # Parse FID score from the output\n",
    "            output = result.stdout\n",
    "            print(output)\n",
    "\n",
    "            for line in output.splitlines():\n",
    "                if \"FID\" in line:\n",
    "                    fid_value = float(line.split()[-1])\n",
    "                    fid_scores.append((epoch+1, fid_value))\n",
    "            if fid_scores:   \n",
    "                clear_output(wait=True)  # Clear previous output\n",
    "                epochs_plot, fids_plot = zip(*fid_scores)  # unzip\n",
    "    \n",
    "                plt.figure(figsize=(8,6))\n",
    "                plt.plot(epochs_plot, fids_plot, marker='o')\n",
    "                plt.title('FID Score vs Epoch')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('FID Score')\n",
    "                plt.grid(True)\n",
    "                plt.show()\n",
    "\n",
    "        netG.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a404ddd7-1c1f-4a48-b518-07e4cb5d0b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], D Loss: 0.4208, G Loss: 1.1118\n",
      "Epoch [2/20], D Loss: 0.5596, G Loss: 1.8297\n",
      "Epoch [3/20], D Loss: 1.2694, G Loss: 2.8514\n",
      "Epoch [4/20], D Loss: 1.1319, G Loss: 1.0446\n",
      "Epoch [5/20], D Loss: 1.0717, G Loss: 1.0177\n",
      "Epoch [6/20], D Loss: 1.0577, G Loss: 1.2817\n",
      "Epoch [7/20], D Loss: 0.9714, G Loss: 0.8473\n",
      "Epoch [8/20], D Loss: 0.9070, G Loss: 1.3033\n",
      "Epoch [9/20], D Loss: 0.9271, G Loss: 1.0514\n",
      "Epoch [10/20], D Loss: 1.1172, G Loss: 0.9994\n",
      "Epoch [11/20], D Loss: 1.0465, G Loss: 0.7948\n",
      "Epoch [12/20], D Loss: 1.1245, G Loss: 1.8056\n",
      "Epoch [13/20], D Loss: 0.9888, G Loss: 1.0640\n",
      "Epoch [14/20], D Loss: 0.8746, G Loss: 1.1976\n",
      "Epoch [15/20], D Loss: 1.3910, G Loss: 1.9657\n",
      "Epoch [16/20], D Loss: 0.9957, G Loss: 1.3943\n",
      "Epoch [17/20], D Loss: 0.9537, G Loss: 1.1455\n",
      "Epoch [18/20], D Loss: 0.8622, G Loss: 1.4306\n",
      "Epoch [19/20], D Loss: 0.8739, G Loss: 1.6641\n",
      "Epoch [20/20], D Loss: 0.7860, G Loss: 1.3203\n",
      "Epoch [1/20], D Loss: 0.7698, G Loss: 0.7929\n",
      "Epoch [2/20], D Loss: 0.7729, G Loss: 0.8856\n",
      "Epoch [3/20], D Loss: 0.9467, G Loss: 1.2504\n",
      "Epoch [4/20], D Loss: 1.4772, G Loss: 0.5160\n",
      "Epoch [5/20], D Loss: 1.3617, G Loss: 0.5021\n",
      "Epoch [6/20], D Loss: 1.3292, G Loss: 0.4854\n",
      "Epoch [7/20], D Loss: 1.2188, G Loss: 0.7105\n",
      "Epoch [8/20], D Loss: 1.0826, G Loss: 1.2517\n",
      "Epoch [9/20], D Loss: 1.1366, G Loss: 0.9317\n",
      "Epoch [10/20], D Loss: 1.0363, G Loss: 0.9137\n",
      "Epoch [11/20], D Loss: 1.1194, G Loss: 0.9781\n",
      "Epoch [12/20], D Loss: 1.0797, G Loss: 1.1041\n",
      "Epoch [13/20], D Loss: 1.1837, G Loss: 0.6827\n",
      "Epoch [14/20], D Loss: 1.1810, G Loss: 0.9722\n",
      "Epoch [15/20], D Loss: 1.0524, G Loss: 0.9384\n",
      "Epoch [16/20], D Loss: 1.1858, G Loss: 1.7672\n",
      "Epoch [17/20], D Loss: 1.0167, G Loss: 0.9816\n",
      "Epoch [18/20], D Loss: 1.0675, G Loss: 0.9232\n",
      "Epoch [19/20], D Loss: 1.1561, G Loss: 1.6127\n",
      "Epoch [20/20], D Loss: 1.0450, G Loss: 1.1493\n",
      "Epoch [1/20], D Loss: 0.2705, G Loss: 2.4968\n",
      "Epoch [2/20], D Loss: 0.5001, G Loss: 1.7207\n",
      "Epoch [3/20], D Loss: 0.5070, G Loss: 1.7015\n",
      "Epoch [4/20], D Loss: 0.8310, G Loss: 1.0920\n",
      "Epoch [5/20], D Loss: 0.9825, G Loss: 1.8323\n",
      "Epoch [6/20], D Loss: 0.9196, G Loss: 1.2339\n",
      "Epoch [7/20], D Loss: 1.0032, G Loss: 0.8248\n",
      "Epoch [8/20], D Loss: 0.9123, G Loss: 1.3400\n",
      "Epoch [9/20], D Loss: 0.8125, G Loss: 1.3524\n",
      "Epoch [10/20], D Loss: 0.9877, G Loss: 1.2986\n",
      "Epoch [11/20], D Loss: 1.0608, G Loss: 1.2472\n",
      "Epoch [12/20], D Loss: 1.0454, G Loss: 1.2560\n",
      "Epoch [13/20], D Loss: 1.2031, G Loss: 1.2902\n",
      "Epoch [14/20], D Loss: 0.9887, G Loss: 1.2899\n",
      "Epoch [15/20], D Loss: 0.8130, G Loss: 1.9467\n",
      "Epoch [16/20], D Loss: 0.8270, G Loss: 1.2905\n",
      "Epoch [17/20], D Loss: 0.8977, G Loss: 1.0148\n",
      "Epoch [18/20], D Loss: 0.9147, G Loss: 1.6758\n",
      "Epoch [19/20], D Loss: 0.8634, G Loss: 1.3050\n",
      "Epoch [20/20], D Loss: 0.7866, G Loss: 1.6372\n",
      "Epoch [1/20], D Loss: 0.3003, G Loss: 1.8627\n",
      "Epoch [2/20], D Loss: 0.5081, G Loss: 1.7490\n",
      "Epoch [3/20], D Loss: 0.6710, G Loss: 0.9190\n",
      "Epoch [4/20], D Loss: 0.8275, G Loss: 0.6793\n",
      "Epoch [5/20], D Loss: 0.8283, G Loss: 1.0038\n",
      "Epoch [6/20], D Loss: 0.9637, G Loss: 1.2983\n",
      "Epoch [7/20], D Loss: 1.0517, G Loss: 1.3681\n",
      "Epoch [8/20], D Loss: 1.0163, G Loss: 1.1882\n",
      "Epoch [9/20], D Loss: 1.0849, G Loss: 1.1622\n",
      "Epoch [10/20], D Loss: 0.9868, G Loss: 0.9057\n",
      "Epoch [11/20], D Loss: 1.0099, G Loss: 1.0369\n",
      "Epoch [12/20], D Loss: 1.2878, G Loss: 0.6411\n",
      "Epoch [13/20], D Loss: 1.0866, G Loss: 1.1518\n",
      "Epoch [14/20], D Loss: 0.9812, G Loss: 0.7936\n",
      "Epoch [15/20], D Loss: 1.1379, G Loss: 0.6580\n",
      "Epoch [16/20], D Loss: 0.9276, G Loss: 1.1936\n",
      "Epoch [17/20], D Loss: 0.9978, G Loss: 0.8237\n",
      "Epoch [18/20], D Loss: 0.9588, G Loss: 0.8696\n",
      "Epoch [19/20], D Loss: 1.0720, G Loss: 0.9155\n",
      "Epoch [20/20], D Loss: 0.9017, G Loss: 1.1919\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Function to initialize weights\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        # Applying a normal distribution with mean=0 and std=0.02 to weights\n",
    "        nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        # Initialize batch norm weights\n",
    "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Then, you can use this function to initialize the weights of your networks.\n",
    "\n",
    "\n",
    "# Parameter ranges to test\n",
    "learning_rates = [0.0002, 0.0001]  # Example values\n",
    "betas = [(0.5, 0.999), (0.3, 0.999)]  # Example values for beta1, beta2\n",
    "epochs = 20  # Adjust according to your needs\n",
    "\n",
    "# Define the training loop\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "fid_scores = [] \n",
    "results = []\n",
    "\n",
    "os.makedirs('/projectnb/ds340/projects/leilani_hannah_final_project/parameter_tuning', exist_ok=True)\n",
    "\n",
    "# Iterate over all combinations of learning rate and beta values\n",
    "for lr, beta1_beta2 in itertools.product(learning_rates, betas):\n",
    "    beta1, beta2 = beta1_beta2  # Unpack beta values\n",
    "    \n",
    "    # Initialize the networks and optimizers for each parameter combination\n",
    "    netG = Generator().to(device)\n",
    "    netD = Discriminator().to(device)\n",
    "    optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "    # Re-initialize weights\n",
    "    netG.apply(weights_init)\n",
    "    netD.apply(weights_init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (real_images, _) in enumerate(dataloader):\n",
    "            batch_size = real_images.size(0)\n",
    "            real_images = real_images.to(device)\n",
    "\n",
    "            real_labels = torch.ones(batch_size).to(device)\n",
    "            fake_labels = torch.zeros(batch_size).to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            netD.zero_grad()\n",
    "            outputs = netD(real_images)\n",
    "            d_loss_real = criterion(outputs, real_labels)\n",
    "\n",
    "            z = torch.randn(batch_size, 100, 1, 1).to(device)\n",
    "            fake_images = netG(z)\n",
    "            outputs = netD(fake_images.detach())\n",
    "            d_loss_fake = criterion(outputs, fake_labels)\n",
    "\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss.backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "            # Train Generator\n",
    "            netG.zero_grad()\n",
    "            z = torch.randn(batch_size, 100, 1, 1).to(device)\n",
    "            fake_images = netG(z)\n",
    "            outputs = netD(fake_images)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "        g_losses.append((epoch + 1, g_loss.item()))\n",
    "        d_losses.append((epoch + 1, d_loss.item()))\n",
    "\n",
    "        # Save fake images for future FID calculation (if you want)\n",
    "        if (epoch + 1) % 5 == 0:  # Every 5 epochs, save fake images for FID calculation\n",
    "            netG.eval()\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(1000, 100, 1, 1).to(device)\n",
    "                fake_images = netG(z)\n",
    "\n",
    "                # Save fake images to the 'parameter_tuning' folder\n",
    "                save_path = '/projectnb/ds340/projects/leilani_hannah_final_project/parameter_tuning/fake_images_epoch_{:03d}_lr_{:g}_beta1_{:g}.npy'.format(epoch+1, lr, beta1)\n",
    "                np.save(save_path, fake_images.cpu().numpy())\n",
    "\n",
    "            netG.train()\n",
    "\n",
    "    # After training for the current set of parameters, you can manually calculate FID for specific epochs if you want\n",
    "    # Append results for parameter combinations\n",
    "    results.append((lr, beta1, g_losses, d_losses))\n",
    "    \n",
    "# You can now manually calculate FID for specific epochs after training and plot them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de2ac9b-0fe4-4474-8d27-1a630a339dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
